---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# NegBinomHMC

<!-- badges: start -->

<!-- badges: end -->

NegBinomHMC is an R package that implements Hamiltonian Monte Carlo (HMC) sampling for Bayesian Negative Binomial regression models. The package provides functions to simulate Negative Binomial data, calculate log-posterior densities and gradients, and perform adaptive HMC sampling with a warmup phase to tune the step size automatically.

## Overview

Negative Binomial regression is commonly used for modeling over-dispersed count data. With NegBinomHMC, you can:

-   **Simulate Data:** Generate synthetic datasets following a Negative Binomial distribution using a specified regression model.

-   **Model Evaluation:** Calculate the log-posterior and its gradient for your Negative Binomial regression model.

-   **Adaptive HMC Sampling:** Sample from the Bayesian posterior using an adaptive HMC sampler that tunes its step size during a warmup period to achieve a desired acceptance rate.

This package is intended for educational purposes and as a starting point for more advanced Bayesian inference in count data models.

## Package Structure

``` bash
NegBinomHMC/
├── DESCRIPTION       # Package metadata
├── NAMESPACE         # Exports information generated by roxygen2
├── R/
│   ├── simulation.R       # Data simulation function
│   ├── negbin_model.R             # Negative Binomial model function
│   ├── hmc_sampler.R                # Adaptive HMC sampler and helper functions
│    ├── rw_metropolis_model.R       # Random walk Metropolis model
├── man/              # Documentation files (generated via roxygen2)
├── README.md         # This file
└── LICENSE           # License file
```

## Installation

You can install the development version of NegBinomHMC from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("Kumachar/NegBinomHMC")
```

## New Features: Dual Averaging for Adaptive Epsilon

### Concept

Dual averaging is a robust method to adapt `epsilon` by targeting a desired acceptance rate (denoted $\delta$, e.g., 0.65). It updates `epsilon` iteratively to minimize the difference between the actual and target acceptance probabilities. The algorithm:

1.  Runs HMC iterations, computing the acceptance probability $\alpha_t$ at iteration $t$.
2.  Updates a running statistic to adjust $\log \epsilon$.
3.  Smooths the updates to stabilize convergence.

The update rules (from Hoffman & Gelman, 2014, used in NUTS) are:

$$
H_t = \left(1 - \frac{1}{t + t_0}\right) H_{t-1} + \frac{1}{t + t_0} (\delta - \alpha_t),
$$

$$
\log \tilde{\epsilon}_t = \mu - \frac{\sqrt{t}}{\gamma} H_t,
$$

and,

$$
\log \epsilon_t = \kappa \log \tilde{\epsilon}_t + (1 - \kappa) \log \epsilon_{t-1},
$$

where:

-   $\alpha_t$: Acceptance probability at iteration $t$.
-   $\delta$: Target acceptance rate (e.g., 0.65).
-   $H_t$: Running statistic tracking the error in acceptance rate.
-   $\mu$: Initial guess for $\log \epsilon$.
-   $t_0$: Controls early iteration weighting (e.g., 10).
-   $\gamma$: Controls adaptation speed (e.g., 0.05).
-   $\kappa$: Smoothing parameter (e.g., 0.75).
-   $\epsilon_t$: Step size at iteration $t$.

After a warm-up phase, we fix `epsilon` to the final adapted value for the sampling phase to ensure Markov chain validity.

## Simulation Example

Below is a basic example demonstrating how to simulate Negative Binomials data and run the adaptive HMC sampler to estimate the regression coefficients.

```{r example, warning=FALSE}
library(NegBinomHMC)
library(coda)

# Set simulation parameters
n <- 200              # Number of observations
p <- 3                # Number of predictors (including intercept)
beta_true <- c(1, 0, -1)  # True regression coefficients
r <- 2                # Dispersion parameter

# Simulate Negative Binomial data
data <- simulate_negbin_data(n, p, beta_true, r, seed = 123)
X <- data$X
y <- data$y

# Run the adaptive HMC sampler
start_time <- Sys.time()
result <- hmc_sampler(
  log_post = log_posterior_negbin,
  grad_log_post = grad_log_posterior_negbin,
  initial_beta = rep(0, p),
  initial_epsilon = 0.01,
  L = 10,
  n_iter = 1000,
  n_warmup = 500,
  X = X,
  y = y,
  r = r,
  beta_mu = 0,
  beta_sigma = 1,
  target_accept = 0.6
)
end_time <- Sys.time()
print(paste("HMC runtime:", end_time - start_time))
cat("True beta:", beta_true, "\n")
cat("Estimated beta (mean):", colMeans(result$samples), "\n")
cat("Acceptance rate:", result$acceptance_rate, "\n")
cat("Final epsilon:", result$final_epsilon, "\n")

# Posterior summaries
posterior_samples <- result$samples
colnames(posterior_samples) <- paste0("beta_", 1:p)
summary(posterior_samples)

#Effective sample size
mcmc_chain <- as.mcmc(posterior_samples)
# Calculate effective sample size for each parameter
ess <- effectiveSize(mcmc_chain)

# Display the effective sample sizes
print("Effective Sample Size:")
print(ess)

# Length of CI
ci_length <- apply(posterior_samples, 2, function(x) {
  quantile(x, 0.975) - quantile(x, 0.025)
})
print("Length of 95% CI for each parameter:")
print( ci_length)


# Trace plot
par(mfrow = c(p, 1))
for (j in 1:p) {
  plot(posterior_samples[, j], type = "l", 
       main = paste("Trace plot for beta", j), ylab = "Value", xlab = "Iteration")
  abline(h = beta_true[j], col = "red", lwd = 2)
}

```
```{r, warning=FALSE}
## Random Walk Metropolis Example
# Prior hyperparameters: using N(0, 10^2) for each beta coefficient.
beta_mu <- 0
beta_sigma <- 1  # In this example, we set sigma=1; adjust as needed.

# Initial beta: starting at zero (vector of length p)
init_beta <- rep(0, p)

# Sampler settings
iterations <- 1000
proposal_sd <- 0.1  # Standard deviation for each proposal increment

# Run the sampler
start_time <- Sys.time()
chain_metropolis <- rw_metropolis(
  log_post    = log_posterior_negbin,
  init        = init_beta,
  iterations  = iterations,
  proposal_sd = proposal_sd,
  X           = X,
  y           = y,
  r           = r,
  beta_mu     = beta_mu,
  beta_sigma  = beta_sigma
)
end_time <- Sys.time()
rwm_runtime <- end_time - start_time
print(paste("Random Walk Metropolis runtime:", rwm_runtime))

#Results for Random-walk Metropolis
cat("Estimated beta (mean):", colMeans(chain_metropolis), "\n")
cat("Acceptance rate:", mean(diff(chain_metropolis) != 0), "\n")

# Posterior summaries
posterior_samples_metropolis <- chain_metropolis
colnames(posterior_samples_metropolis) <- paste0("beta_", 1:p)
summary(posterior_samples_metropolis)
# Trace plot
par(mfrow = c(p, 1))

# Effective sample size
mcmc_chain <- as.mcmc(chain_metropolis)

# Length of CI

ci_length <- apply(posterior_samples_metropolis, 2, function(x) {
  quantile(x, 0.975) - quantile(x, 0.025)
})
print("Length of 95% CI for each parameter:")
print( ci_length)

# Calculate effective sample size for each parameter
print("Effective Sample Size:")
ess <- effectiveSize(mcmc_chain)

# Display the effective sample sizes
print(ess)


for (j in 1:p) {
  plot(posterior_samples_metropolis[, j], type = "l", 
       main = paste("Trace plot for beta", j), ylab = "Value", xlab = "Iteration")
  abline(h = beta_true[j], col = "red", lwd = 2)
}
```

## Real-World Example 1

In this analysis, we investigate the factors influencing the number of days of absence among high school juniors. The data come from a study involving school administrators, where the predictors include the type of program in which a student is enrolled and a standardized math test score. Due to possible overdispersion in the count data, a Negative Binomial model is appropriate compared to a Poisson model.

We use our HMC package (NegBinomHMC) to sample from the posterior distribution of a Negative Binomial regression model. This document details the steps for reading in the data, building the design matrix, running the HMC sampler, and comparing the results with a baseline model.

### Data Reading and Preprocessing

We start by reading the dataset using the `haven` package. The dataset is stored in a Stata file and contains variables such as `math` (standardized test score), `prog` (program type), and `daysabs` (number of absence days). We convert the program type to a factor variable for modeling.

```{r real-world-example-1, warning=FALSE}
library(haven)
library(NegBinomHMC)
dat <- read_dta("https://stats.idre.ucla.edu/stat/stata/dae/nb_data.dta")
dat <- within(dat, {
  prog <- factor(prog, levels = 1:3, labels = c("General", "Academic", "Vocational"))
  id <- factor(id)
})

head(dat)

X <- model.matrix(~ math + prog, data = dat)
y <- dat$daysabs
r <- 2
p <- ncol(X)
# Define the log-posterior function for the negative binomial model
# Fit a negative binomial regression model

result <- hmc_sampler(
  log_post = log_posterior_negbin,
  grad_log_post = grad_log_posterior_negbin,
  initial_beta = rep(0, p),
  initial_epsilon = 0.01,
  L = 10,
  n_iter = 1000,
  n_warmup = 500,
  X = X,
  y = y,
  r = r,
  beta_mu = 0,
  beta_sigma = 1,
  target_accept = 0.6
)

cat("Estimated beta (mean):", colMeans(result$samples), "\n")
cat("Acceptance rate:", result$acceptance_rate, "\n")
cat("Final epsilon:", result$final_epsilon, "\n")
```

## Real-World Example 2

In this analysis, we investigate the factors influencing the total number of injuries in traffic accidents. The data come from a traffic accidents dataset and include predictors such as lighting condition, number of units involved, day of the week, and month of the crash. Considering possible overdispersion in the count data, we use a Negative Binomial regression model. This document demonstrates how to fit the model using both a traditional approach (via `glm.nb` from the MASS package) and our custom Hamiltonian Monte Carlo (HMC) sampler.

### Data Preprocessing

We start by reading in the `traffic_accidents.csv` file. The categorical variables are converted to factors, and numerical values (e.g., number of units) are properly set as numeric.

```{r real-world-example-2, warning=FALSE}
library(NegBinomHMC)
accidents <- read.csv("ExampleData/traffic_accidents.csv", stringsAsFactors = TRUE)

accidents$weather_condition <- as.factor(accidents$weather_condition)
accidents$lighting_condition <- as.factor(accidents$lighting_condition)
accidents$crash_day_of_week <- as.factor(accidents$crash_day_of_week)
accidents$crash_month <- as.factor(accidents$crash_month)
accidents$num_units <- as.numeric(as.character(accidents$num_units))

library(MASS)
nb_model <- glm.nb(injuries_total ~ lighting_condition + num_units +
                     crash_day_of_week + crash_month,
                   data = accidents)
summary(nb_model)

nb_formula <- injuries_total ~ lighting_condition + num_units + crash_day_of_week + crash_month
X <- model.matrix(nb_formula, data = accidents)
y <- accidents$injuries_total

r <- 0.74
beta_mu <- rep(0, ncol(X))
beta_sigma <- rep(10, ncol(X))

initial_beta <- rep(0, ncol(X))

initial_epsilon <- 0.1 
L <- 10 
n_iter <- 1000
n_warmup <- 500
target_accept <- 0.65

hmc_result <- hmc_sampler(
  log_post = log_posterior_negbin,
  grad_log_post = grad_log_posterior_negbin,
  initial_beta = initial_beta,
  initial_epsilon = initial_epsilon,
  L = L,
  n_iter = n_iter,
  n_warmup = n_warmup,
  X = X,
  y = y,
  r = r,
  beta_mu = beta_mu,
  beta_sigma = beta_sigma,
  target_accept = target_accept
)

samples <- hmc_result$samples
accept_rate <- hmc_result$acceptance_rate
final_epsilon <- hmc_result$final_epsilon

cat("HMC acceptance rate:", accept_rate, "\n")
cat("final epsilon:", final_epsilon, "\n")

posterior_means <- colMeans(samples)
cat("HMC posterior mean：\n")
print(posterior_means)

cat("glm model estimated coefficients：\n")
print(coef(nb_model))

ci_length <- apply(samples, 2, function(x) {
  quantile(x, 0.975) - quantile(x, 0.025)
})
print("Length of 95% CI for each parameter:")
print(ci_length)


# Metropolis model
## Random Walk Metropolis Example
# Prior hyperparameters: using N(0, 10^2) for each beta coefficient.
beta_mu <- 0
beta_sigma <- 1  # In this example, we set sigma=1; adjust as needed.

# Initial beta: starting at zero (vector of length p)
proposal_sd <- 0.1
iterations <- 1000

chain_metropolis <- rw_metropolis(
  log_post = log_posterior_negbin,
  init = initial_beta,
  iterations = iterations,
  proposal_sd = proposal_sd,
  X = X,
  y = y,
  r = r,
  beta_mu = beta_mu,
  beta_sigma = beta_sigma
)

cat("Random Walk Metropolis acceptance rate:", mean(diff(chain_metropolis) != 0), "\n")
cat("Random Walk Metropolis posterior mean：\n")
print(colMeans(chain_metropolis))

ci_length <- apply(chain_metropolis, 2, function(x) {
  quantile(x, 0.975) - quantile(x, 0.025)
})
print("Length of 95% CI for each parameter:")
print( ci_length)

# Load necessary libraries
library(ggplot2)

# --- Assign column names if they are missing ---
if (is.null(colnames(samples)) || length(colnames(samples)) == 0) {
  colnames(samples) <- colnames(X)
}
if (is.null(colnames(chain_metropolis)) || length(colnames(chain_metropolis)) == 0) {
  colnames(chain_metropolis) <- colnames(X)
}

# ===== Compute HMC estimates and CI =====
# Posterior means from your HMC sampling:
hmc_mean <- colMeans(samples)

# Compute the HMC 95% credible intervals using the 2.5% and 97.5% quantiles:
hmc_ci_lower <- apply(samples, 2, quantile, probs = 0.025)
hmc_ci_upper <- apply(samples, 2, quantile, probs = 0.975)


# --- Compute Random Walk Metropolis summaries ---
rwm_mean    <- colMeans(chain_metropolis)
rwm_ci_lower <- apply(chain_metropolis, 2, quantile, probs = 0.025)
rwm_ci_upper <- apply(chain_metropolis, 2, quantile, probs = 0.975)

# ===== Compute GLM estimates and CI =====
# Coefficients from the NB model:
glm_coef <- coef(nb_model)

# Compute the 95% confidence intervals for the NB model (profile likelihood-based by default):
glm_ci <- confint(nb_model)
# glm_ci is a matrix with two columns (lower, upper). Its rownames should match the parameter names.

# ===== Prepare a data frame for plotting =====
# Define parameter names from the design matrix (or the samples)
params <- colnames(X)

# Create the data frame for plotting:
df_combined <- data.frame(
  Parameter = rep(params, 3),
  Method = factor(rep(c("HMC", "GLM", "RWM"), each = length(params))),
  Estimate = c(hmc_mean, glm_coef, rwm_mean),
  CI_Lower = c(hmc_ci_lower, glm_ci[, 1], rwm_ci_lower),
  CI_Upper = c(hmc_ci_upper, glm_ci[, 2], rwm_ci_upper)
)


# Optionally, order the parameter factor to preserve the original order:
df_combined$Parameter <- factor(df_combined$Parameter, levels = params)



# ===== Create the plot =====
# Use ggplot2 to plot point estimates and error bars for the 95% intervals.
ggplot(df_combined, aes(x = Parameter, y = Estimate, color = Method)) +
  geom_point(position = position_dodge(width = 0.6), size = 3) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper),
                width = 0.2,
                position = position_dodge(width = 0.6)) +
  theme_bw() +
  labs(title = "Parameter Estimates with 95% Intervals",
       x = "Parameter",
       y = "Coefficient Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


first_three <- params[1:3]

# --- Set up the plotting area: 2 rows (first row for HMC, second row for RWM), 3 columns ---
par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))

# --- Generate trace plots for HMC and add mean line ---
for (i in 1:3) {
  param_name <- first_three[i]
  plot(samples[, param_name],
       type = "l",
       main = paste("HMC Trace:", param_name),
       xlab = "Iteration",
       ylab = "Value")
  
  # Compute the mean for HMC for this parameter
  hmc_mean_val <- mean(samples[, param_name])
  
  # Add a horizontal dashed line at the mean (red)
  abline(h = hmc_mean_val, col = "red", lwd = 2, lty = 2)
  
  # Add a legend indicating the mean value
  legend("topright", legend = paste("Mean:", round(hmc_mean_val, 3)), 
         bty = "n", col = "red", lty = 2, cex = 0.8)
}

# --- Generate trace plots for RWM and add mean line ---
for (i in 1:3) {
  param_name <- first_three[i]
  plot(chain_metropolis[, param_name],
       type = "l",
       main = paste("RWM Trace:", param_name),
       xlab = "Iteration",
       ylab = "Value")
  
  # Compute the mean for RWM for this parameter
  rwm_mean_val <- mean(chain_metropolis[, param_name])
  
  # Add a horizontal dashed line at the mean (blue)
  abline(h = rwm_mean_val, col = "blue", lwd = 2, lty = 2)
  
  # Add a legend indicating the mean value
  legend("topright", legend = paste("Mean:", round(rwm_mean_val, 3)), 
         bty = "n", col = "blue", lty = 2, cex = 0.8)
}

# Reset the plotting layout to one plot per window
par(mfrow = c(1, 1))
```


## Contributing

Contributions to NegBinomHMC are welcome. If you encounter issues or have suggestions for improvements, please feel free to open an issue or submit a pull request on the GitHub repository.

## License

This project is licensed under the MIT License. See the LICENSE file for details.

## Contact

For questions or further information, please contact Weixiong at [[wxhua\@umich.edu](mailto:wxhua@umich.edu){.email}].
